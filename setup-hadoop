#!/bin/bash

# https://rstudio-pubs-static.s3.amazonaws.com/78508_abe89197267240dfb6f4facb361a20ed.html

# Function for asking to proceed
confirm () {
  read -r -p "${1:-Are you sure? [y/N]} " response
  case $response in
    [yY][eE][sS]|[yY])
      true
      ;;
     *)
      false
      ;;
  esac
}

function setup_data_drive_01 {

  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdc
  sudo mkfs -t ext3 /dev/sdc1
  sudo mkdir /data
  sudo mount /dev/sdc1 /data
}

function setup_data_drive_16 {

  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdc
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdd
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sde
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdf
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdg
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdh
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdi
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdj
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdk
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdl
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdm
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdn
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdo
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdp
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdq
  echo -e "n\np\n1\n\n\nw" | sudo fdisk /dev/sdr

  sudo mdadm --create /dev/md127 --level 0 --raid-devices 16 /dev/sdc1 /dev/sdd1 /dev/sde1 /dev/sdf1 /dev/sdg1 /dev/sdh1 /dev/sdi1 /dev/sdj1 /dev/sdk1 /dev/sdl1 /dev/sdm1 /dev/sdn1 /dev/sdo1 /dev/sdp1 /dev/sdq1 /dev/sdr1
  sudo mkfs -t ext4 /dev/md127

  sudo mkdir /data
  sudo mount /dev/md127 /data
}

function update_apt_repo  {
  sudo add-apt-repository ppa:webupd8team/java
  sudo apt-get update 
  sudo apt-get install mdadm bonnie++ iozone3
  sudo apt-get install openssh-server
} 

function install_java {
  sudo apt-get install oracle-java8-installer
  sudo apt-get install oracle-java8-set-default
}

function create_hadoop_user {

	sudo addgroup hadoop
	sudo adduser --ingroup hadoop hduser
	sudo adduser hduser sudo
	sudo chsh -s /bin/bash hduser
	sudo passwd hduser  
}

function install_hadoop {

	wget http://www-us.apache.org/dist/hadoop/core/stable/hadoop-2.7.4.tar.gz
	tar xfz hadoop-2.7.4.tar.gz
	mv hadoop-2.7.4 /usr/local/hadoop
}

function setup_environment {

echo "Now script is updating Bashrc for export Path etc"  

cat >> ~/.bashrc << EOL
export JAVA_HOME=/usr/lib/jvm/java-8-oracle/
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native  
EOL

cat ~/.bashrc

source ~/.bashrc


echo "Now script is updating hadoop-env.sh"  

cat >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh << EOL
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
EOL

cat /usr/local/hadoop/etc/hadoop/hadoop-env.sh


echo "Now script is updating core-site.xml"  

cat >> /usr/local/hadoop/etc/hadoop/core-site.xml << EOL
<configuration>
<property><name>fs.default.name</name><value>hdfs://localhost:9000</value></property>
</configuration>  
EOL


echo "Now script is updating yarn-site.xml"  

cat >> /usr/local/hadoop/etc/hadoop/yarn-site.xml << EOL
<configuration>
<property><name>yarn.nodemanager.aux-services</name><value>mapreduce_shuffle</value></property>
<property><name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name><value>org.apache.hadoop.mapred.ShuffleHandler</value></property>
</configuration>
EOL


echo "Now script is updating mapred-site.xml"

cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

cat >> /usr/local/hadoop/etc/hadoop/mapred-site.xml << EOL
<configuration>
<property><name>mapreduce.framework.name</name><value>yarn</value></property>
</configuration>
EOL


echo "Now script is updating hdfs-site.xml"

sudo mkdir -p /data/hadoop_store/hdfs/namenode
sudo mkdir -p /data/hadoop_store/hdfs/datanode

cat >> /usr/local/hadoop/etc/hadoop/hdfs-site.xml << EOL
<configuration>
<property><name>dfs.replication</name><value>1</value></property>
<property><name>dfs.namenode.name.dir</name><value>file:/data/hadoop_store/hdfs/namenode</value></property>
<property><name>dfs.datanode.data.dir</name><value>file:/data/hadoop_store/hdfs/datanode</value></property>
</configuration>
EOL

rm -rf /data/hadoop_store
sudo chown hduser:hadoop -R /data/hadoop_store
sudo chmod 777 -R /data/hadoop_store


echo "Completed process Now Reloading Bash Profile...."  
cd ~  

echo "You may require reloading bash profile, you can reload using following command."  
echo "source ~/.bashrc"  

echo "To Start you need to format Name Node Once you can use following command."  
echo "hdfs namenode -format"  

echo "Hadoop configured. now you can start hadoop using following commands. "  
echo "start-dfs.sh"  
echo "start-yarn.sh"  

echo "To stop hadoop use following scripts."  
echo "su hduser"  
echo "cd /usr/local/hadoop/"
echo "stop-dfs.sh"  
echo "stop-yarn.sh"  
echo "jps"  
}

clear

confirm "Setup Data Drive 01? [y/N] " && setup_data_drive_01
confirm "Setup Data Drive 16? [y/N] " && setup_data_drive_16
confirm "Update Repository? [y/N] " && update_apt_repo 
confirm "Install Java? [y/N] " && install_java
confirm "Create hadoop user? [y/N] " && create_hadoop_user
confirm "Install hadoop? [y/N] " && install_hadoop 
confirm "Setup Hadoop Environment? [y/N] " &&  setup_environment
